{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJLkpj7Qjetw"
      },
      "source": [
        "# Load Dependency\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mHNeWIkMG-U",
        "outputId": "b62f0e11-e623-4366-a5b3-a103cfef69c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "glove.840B.300d.zip\n"
          ]
        }
      ],
      "source": [
        "# Make sure the glove.840B.300d.zip is in the project directory under .vector_cache\n",
        "# Only use this in Colab if you have the zip file and want to copy over\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!cp -r drive/MyDrive/CompSci-590-NLP/Final\\ Project/.vector_cache .\n",
        "!ls .vector_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBk_hT2QBIgF"
      },
      "outputs": [],
      "source": [
        "# This only needs to be executed once unless you don't see the WikiSQL folder on the left\n",
        "\n",
        "!git clone https://github.com/salesforce/WikiSQL\n",
        "!pip install -r WikiSQL/requirements.txt\n",
        "!tar xvjf WikiSQL/data.tar.bz2\n",
        "\n",
        "!pip install graph4nlp-cu110\n",
        "\n",
        "# Comment back this if need customized setup, meanwhile comment out last line\n",
        "# !pip3 install PyYAML\n",
        "# !pip3 install nltk\n",
        "# !pip3 install scipy\n",
        "# !git clone https://github.com/graph4ai/graph4nlp.git\n",
        "# !graph4nlp/configure\n",
        "# !python graph4nlp/setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCYJ1DSB88S",
        "outputId": "c24e6830-79fe-4a7c-e2aa-ebce630df593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        }
      ],
      "source": [
        "# import all libraries\n",
        "\n",
        "import json\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from random import sample\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from graph4nlp.pytorch.data.data import to_batch, from_batch\n",
        "from graph4nlp.pytorch.data import GraphData\n",
        "from graph4nlp.pytorch.data.dataset import Text2TextDataItem as DataItem\n",
        "from graph4nlp.pytorch.modules.utils.vocab_utils import VocabModel\n",
        "from graph4nlp.pytorch.modules.config import get_basic_args\n",
        "from graph4nlp.pytorch.models.graph2seq import Graph2Seq\n",
        "from graph4nlp.pytorch.models.graph2seq_loss import Graph2SeqLoss\n",
        "from graph4nlp.pytorch.modules.evaluation.base import EvaluationMetricBase\n",
        "\n",
        "from graph4nlp.pytorch.modules.utils.padding_utils import pad_2d_vals_no_size\n",
        "\n",
        "from graph4nlp.pytorch.modules.utils.copy_utils import prepare_ext_vocab\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "embed_dim = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSm_ISUliTXp",
        "outputId": "b6dc0764-0eb1-469e-921d-fa15a4addfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "11.1\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qP0OGAejvEE"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3N218ssjzw9"
      },
      "source": [
        "## Load tables\n",
        "\n",
        "Load tables for later lookups when parsing queries. Only need to be run once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IGGNp3nbHEFn"
      },
      "outputs": [],
      "source": [
        "# Store table in a dict: id --> json\n",
        "f_train = open('data/train.tables.jsonl')\n",
        "train_tables = {json.loads(e)['id']:json.loads(e) for e in f_train}\n",
        "f_train.close()\n",
        "\n",
        "f_dev = open('data/dev.tables.jsonl')\n",
        "dev_tables = {json.loads(e)['id']:json.loads(e) for e in f_dev}\n",
        "f_dev.close()\n",
        "\n",
        "f_test = open('data/test.tables.jsonl')\n",
        "test_tables = {json.loads(e)['id']:json.loads(e) for e in f_test}\n",
        "f_test.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bawv21pj39a"
      },
      "source": [
        "## Parse Query Graph\n",
        "\n",
        "This section define the function for parsing queries to graphs. Only need to be run once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I7GozwbQ-gbS"
      },
      "outputs": [],
      "source": [
        "# build the graph given a query\n",
        "\n",
        "def _parse_query_graph(q, table):\n",
        "    g = GraphData()\n",
        "    cnt = 0\n",
        "\n",
        "    # node for select\n",
        "    g.add_nodes(1)\n",
        "    g.node_attributes[cnt]['token'] = 'SELECT'\n",
        "    cnt += 1\n",
        "\n",
        "    # node for agg\n",
        "    if q['agg']:\n",
        "        g.add_nodes(1)\n",
        "        g.node_attributes[cnt]['token'] = agg_ops[q['agg']]\n",
        "        g.add_edge(cnt - 1, cnt)\n",
        "        cnt += 1\n",
        "        \n",
        "\n",
        "    # selected column\n",
        "    g.add_nodes(1)\n",
        "    g.node_attributes[cnt]['token'] = table['header'][q['sel']]\n",
        "    g.add_edge(cnt - 1, cnt)\n",
        "    cnt += 1\n",
        "\n",
        "\n",
        "    # FROM node\n",
        "    g.add_nodes(1)\n",
        "    g.node_attributes[cnt]['token'] = 'FROM'\n",
        "    g.add_edge(cnt - 1, cnt)\n",
        "    cnt += 1\n",
        "\n",
        "    # table node\n",
        "    g.add_nodes(1)\n",
        "    g.node_attributes[cnt]['token'] = 'table'\n",
        "    g.add_edge(cnt - 1, cnt)\n",
        "    cnt += 1\n",
        "\n",
        "    # WHERE node\n",
        "    g.add_nodes(1)\n",
        "    g.node_attributes[cnt]['token'] = 'WHERE'\n",
        "    g.add_edge(cnt - 1, cnt)\n",
        "    cnt += 1\n",
        "\n",
        "    # need 'and' node or not\n",
        "    if len(q['conds']) > 1:\n",
        "        and_idx = cnt\n",
        "\n",
        "        g.add_nodes(1)\n",
        "        g.node_attributes[cnt]['token'] = 'AND'\n",
        "        g.add_edge(cnt - 1, cnt)\n",
        "        cnt += 1\n",
        "\n",
        "        for cond in q['conds']:\n",
        "            g.add_nodes(1)\n",
        "            g.node_attributes[cnt]['token'] = table['header'][cond[0]]\n",
        "            g.add_edge(and_idx, cnt)\n",
        "            cnt += 1\n",
        "\n",
        "            g.add_nodes(1)\n",
        "            g.node_attributes[cnt]['token'] = cond_ops[cond[1]]\n",
        "            g.add_edge(cnt - 1, cnt)\n",
        "            cnt += 1\n",
        "\n",
        "            g.add_nodes(1)\n",
        "            g.node_attributes[cnt]['token'] = str(cond[2])\n",
        "            g.add_edge(cnt - 1, cnt)\n",
        "            cnt += 1\n",
        "    elif len(q['conds']) == 1:\n",
        "        # order: col, op, const\n",
        "        g.add_nodes(1)\n",
        "        g.node_attributes[cnt]['token'] = table['header'][q['conds'][0][0]]\n",
        "        g.add_edge(cnt - 1, cnt)\n",
        "        cnt += 1\n",
        "\n",
        "        g.add_nodes(1)\n",
        "        g.node_attributes[cnt]['token'] = cond_ops[q['conds'][0][1]]\n",
        "        g.add_edge(cnt - 1, cnt)\n",
        "        cnt += 1\n",
        "\n",
        "        g.add_nodes(1)\n",
        "        g.node_attributes[cnt]['token'] = str(q['conds'][0][2])\n",
        "        g.add_edge(cnt - 1, cnt)\n",
        "        cnt += 1\n",
        "\n",
        "    g.node_features['token_id'] = torch.zeros(cnt, 1, dtype=torch.long)\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "def _parse_query_json(q, table):\n",
        "    query = 'SELECT {agg} {sel} FROM table'.format(\n",
        "                agg=agg_ops[q['agg']],\n",
        "                sel=table['header'][q['sel']],\n",
        "            ) if q['agg'] else \\\n",
        "            'SELECT {sel} FROM table'.format(\n",
        "                sel=table['header'][q['sel']],\n",
        "            )\n",
        "    if q['conds']:\n",
        "        query += ' WHERE ' + ' AND '.join(['{} {} {}'.format(table['header'][i], cond_ops[o], v) for i, o, v in q['conds']])\n",
        "    return query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl9uFX09kJDT"
      },
      "source": [
        "## Vocab and Graph for Train & Dev\n",
        "\n",
        "This section build the vocab and parse data into train, dev and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DxHhLbSAjf8",
        "outputId": "84e6c2c9-2835-45a0-f8f5-5a8c3fd35994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/graph4nlp/pytorch/modules/utils/vocab_utils.py:105: UserWarning: Warning: use separate vocabularies for source and target language but samepretrained word embeddings\n",
            "  \"Warning: use separate vocabularies for source and target language but same\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained word embeddings hit ratio: 0.9030159668835009\n",
            "Using pretrained word embeddings\n",
            "Pretrained word embeddings hit ratio: 0.9116675839295542\n",
            "Using pretrained word embeddings\n",
            "[ Using separate word vocabs for input & output text ]\n",
            "[ Initialized input word embeddings: (3382, 300) ]\n",
            "[ Initialized output word embeddings: (3634, 300) ]\n"
          ]
        }
      ],
      "source": [
        "SHARE_VOCAB = False\n",
        "\n",
        "# Build training dataset and vocab\n",
        "train_data_set = []\n",
        "\n",
        "agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
        "cond_ops = ['=', '>', '<', 'OP']\n",
        "\n",
        "\n",
        "with open('data/train.jsonl') as f_train:\n",
        "    for line in f_train:\n",
        "        qjson = json.loads(line)\n",
        "        tid = qjson['table_id']\n",
        "\n",
        "        if qjson['question'][-1] in ['?', '!', '.']:\n",
        "            qjson['question'] = qjson['question'][:-1]\n",
        "\n",
        "        query = _parse_query_json(qjson['sql'], train_tables[tid])\n",
        "        g = _parse_query_graph(qjson['sql'], train_tables[tid])\n",
        "        g.graph_attributes['query'] = query\n",
        "        d = DataItem(query, qjson['question'], None, share_vocab=SHARE_VOCAB)\n",
        "        d.graph = g\n",
        "\n",
        "        train_data_set.append(d)\n",
        "\n",
        "\n",
        "vocab_model = VocabModel(data_set=train_data_set, \n",
        "                         tokenizer=None, \n",
        "                         lower_case=False, \n",
        "                         max_word_vocab_size=None, \n",
        "                         min_word_vocab_freq=10, \n",
        "                         pretrained_word_emb_name='840B', \n",
        "                         word_emb_size=embed_dim, \n",
        "                         share_vocab=SHARE_VOCAB)\n",
        "\n",
        "\n",
        "# fill initial embedding for each node\n",
        "for d in train_data_set:\n",
        "    for i in range(len(d.graph.node_attributes)):\n",
        "        token_id = vocab_model.in_word_vocab.getIndex(d.graph.node_attributes[i]['token'])\n",
        "        d.graph.node_features['token_id'][i][0] = token_id\n",
        "\n",
        "\n",
        "# Build dev dataset\n",
        "dev_dataset = []\n",
        "\n",
        "with open('data/dev.jsonl') as f_dev:\n",
        "    for line in f_dev:\n",
        "        qjson = json.loads(line)\n",
        "        tid = qjson['table_id']\n",
        "        query = _parse_query_json(qjson['sql'], dev_tables[tid])\n",
        "        g = _parse_query_graph(qjson['sql'], dev_tables[tid])\n",
        "        g.graph_attributes['query'] = query\n",
        "        d = DataItem(query, qjson['question'], None, share_vocab=SHARE_VOCAB)\n",
        "        d.graph = g\n",
        "        dev_dataset.append(d)\n",
        "\n",
        "        for i in range(len(d.graph.node_attributes)):\n",
        "            token_id = vocab_model.in_word_vocab.getIndex(d.graph.node_attributes[i]['token'])\n",
        "            d.graph.node_features['token_id'][i][0] = token_id\n",
        "\n",
        "\n",
        "# Build test dataset\n",
        "test_dataset = []\n",
        "\n",
        "with open('data/test.jsonl') as f_test:\n",
        "  for line in f_test:\n",
        "    qjson = json.loads(line)\n",
        "    tid = qjson['table_id']\n",
        "    query = _parse_query_json(qjson['sql'], test_tables[tid])\n",
        "    g = _parse_query_graph(qjson['sql'], test_tables[tid])\n",
        "    g.graph_attributes['query'] = query\n",
        "    d = DataItem(query, qjson['question'], None, share_vocab=True)\n",
        "    d.graph = g\n",
        "    test_dataset.append(d)\n",
        "\n",
        "    for i in range(len(d.graph.node_attributes)):\n",
        "      token_id = vocab_model.in_word_vocab.getIndex(d.graph.node_attributes[i]['token'])\n",
        "      d.graph.node_features['token_id'][i][0] = token_id\n",
        "#       embed = vocab_model.in_word_vocab.embeddings[token_id]\n",
        "#       d.graph.node_features['node_feat'][i] = torch.Tensor(embed)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UOMhXXH2qGKj"
      },
      "outputs": [],
      "source": [
        "# dataset class\n",
        "class SQL2TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, glist, vocab_model):\n",
        "        self.glist = glist\n",
        "        self.vocab_model = vocab_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.glist)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        x = self.glist[i]\n",
        "        y = self.vocab_model.out_word_vocab.to_index_sequence(self.glist[i].output_text) + [2] # sos, ..., eos\n",
        "        return x, y\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data_list):\n",
        "        # print(data_list)\n",
        "        graph_list = [d[0].graph for d in data_list]\n",
        "        query_list = [d[0].graph.graph_attributes['query'] for d in data_list]\n",
        "        graph_data = to_batch(graph_list)\n",
        "\n",
        "        np_output = [np.array(d[1], dtype=np.int32) for d in data_list]\n",
        "        output_str = [deepcopy(d[0].output_text.strip()) for d in data_list]\n",
        "        output_pad = pad_2d_vals_no_size(np_output)\n",
        "      \n",
        "        output_idx = torch.from_numpy(output_pad).long()\n",
        "        return {'graph': graph_data, 'output_idx': output_idx, 'output_str': output_str, 'query': query_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FB1b54Pkc6a"
      },
      "source": [
        "# Graph2Seq Model Setting\n",
        "\n",
        "This section defines the setting for Graph2Seq model. You need to run it whenever you decide to change the setting, and also rerun the SQL2Text Model block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uhlU1tCA-wAX"
      },
      "outputs": [],
      "source": [
        "opt = {\n",
        "  \"graph_construction_args\": {\n",
        "    \"graph_construction_share\": {\n",
        "      \"graph_type\": \"node_emb\",\n",
        "      \"root_dir\": None,\n",
        "      \"topology_subdir\": \"NodeEmbGraph\",\n",
        "      \"share_vocab\": SHARE_VOCAB\n",
        "    },\n",
        "    \"graph_construction_private\": {\n",
        "      \"lower_case\": False\n",
        "    },\n",
        "    \"node_embedding\": {\n",
        "      \"input_size\": 300,\n",
        "      \"hidden_size\": 300,\n",
        "      \"word_dropout\": 0,\n",
        "      \"rnn_dropout\": 0.3,\n",
        "      \"fix_bert_emb\": False,\n",
        "      \"fix_word_emb\": False,\n",
        "      \"embedding_style\": {\n",
        "        \"single_token_item\": True,\n",
        "        \"emb_strategy\": \"w2v\",\n",
        "        \"num_rnn_layers\": 1,\n",
        "        \"bert_model_name\": None,\n",
        "        \"bert_lower_case\": None\n",
        "      },\n",
        "      \"sim_metric_type\": \"weighted_cosine\",\n",
        "      \"num_heads\": 1,\n",
        "      \"top_k_neigh\": 8,\n",
        "    #   \"epsilon_neigh\": None,\n",
        "    #   \"smoothness_ratio\": 0.1,\n",
        "    #   \"connectivity_ratio\": 0.05,\n",
        "    #   \"sparsity_ratio\": 0.1\n",
        "    }\n",
        "  },\n",
        "  \"graph_embedding_args\": {\n",
        "    \"graph_embedding_share\": {\n",
        "      \"num_layers\": 3,\n",
        "      \"input_size\": 300,\n",
        "      \"hidden_size\": 300,\n",
        "      \"output_size\": 300,\n",
        "      \"direction_option\": \"bi_sep\",\n",
        "      \"feat_drop\": 0.3\n",
        "    },\n",
        "    \"graph_embedding_private\": {\n",
        "      \"heads\": [\n",
        "        10, 10, 10\n",
        "      ],\n",
        "      \"attn_drop\": 0.2,\n",
        "      \"negative_slope\": 0.2,\n",
        "      \"residual\": False,\n",
        "      \"activation\": \"relu\",\n",
        "      \"allow_zero_in_degree\": False\n",
        "    }\n",
        "  },\n",
        "  \"decoder_args\": {\n",
        "    \"rnn_decoder_share\": {\n",
        "      \"rnn_type\": \"lstm\",\n",
        "      \"input_size\": 300,\n",
        "      \"hidden_size\": 512,\n",
        "      \"rnn_emb_input_size\": 300,\n",
        "      \"use_copy\": False,\n",
        "      \"use_coverage\": True,\n",
        "      \"graph_pooling_strategy\": \"max\",\n",
        "      \"attention_type\": \"uniform\",\n",
        "      \"fuse_strategy\": \"average\",\n",
        "      \"dropout\": 0.4\n",
        "    },\n",
        "    \"rnn_decoder_private\": {\n",
        "      \"max_decoder_step\": 50,\n",
        "      \"node_type_num\": None,\n",
        "      \"tgt_emb_as_output_layer\": False,\n",
        "      \"teacher_forcing_rate\": 1\n",
        "    }\n",
        "  },\n",
        "  \"graph_construction_name\": \"node_emb\",\n",
        "  \"graph_embedding_name\": \"graphsage\",\n",
        "  \"decoder_name\": \"stdrnn\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NY0JlXJlgaQ"
      },
      "source": [
        "# Reconstruction Seq2Seq Model\n",
        "\n",
        "Define the reconstruction model. Only need to be run once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KuZLInqLlkVz"
      },
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = vocab_model.in_word_vocab.PAD\n",
        "TRG_PAD_IDX = vocab_model.out_word_vocab.PAD\n",
        "SOURCE = vocab_model.in_word_vocab\n",
        "TARGET = vocab_model.out_word_vocab\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention\n",
        "    \n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class AttentionPointerDecoderV3(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 copy=True,\n",
        "                 source_field = SOURCE,\n",
        "                 target_field = TARGET,\n",
        "                 src_pad_idx=SRC_PAD_IDX, \n",
        "                 trg_pad_idx=TRG_PAD_IDX, \n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        # self.tok_embedding = nn.Embedding.from_pretrained(target_field.vocab.vectors, freeze=False)\n",
        "        self.tok_embedding = nn.Embedding.from_pretrained(torch.from_numpy(target_field.embeddings), freeze=False)\n",
        "        \n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.copy = copy\n",
        "        self.output_dim = output_dim\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.source_field = source_field\n",
        "        self.target_field = target_field\n",
        "        #self.fc_nhead_to_one = nn.Linear(n_heads, 1)\n",
        "        self.fc_nhead_to_one = nn.Sequential(\n",
        "          nn.Linear(n_heads, 64),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(64, 64),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(64, 1)\n",
        "        )\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # adapted\n",
        "        self.tp_itos = [self.source_field.getWord(i) for i in range(self.source_field.get_vocab_size())]\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask, src):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        untouched_src = src.clone()\n",
        "        untounched_trg = trg.clone()\n",
        "        \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "        # print(self.target_field.embeddings.shape)\n",
        "        # print(trg)\n",
        "        # for x in trg:\n",
        "        #     for y in x:\n",
        "        #         if y >= 5511:\n",
        "        #             print(y, 'out of range')\n",
        "        # self.tok_embedding(trg)\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        # attention copy kicks in\n",
        "        if self.copy:\n",
        "            # sum each head attention\n",
        "            in_att = attention.permute(1, 0, 2, 3)\n",
        "            in_att = in_att.view(self.n_heads, untounched_trg.shape[0], untounched_trg.shape[1] * untouched_src.shape[1])\n",
        "            in_att = in_att.reshape(self.n_heads, untounched_trg.shape[0] * untounched_trg.shape[1] * untouched_src.shape[1])\n",
        "            in_att = in_att.permute(1, 0)\n",
        "            \n",
        "            alpha = self.fc_nhead_to_one(in_att)\n",
        "            alpha = alpha.permute(1, 0)\n",
        "            alpha = alpha.view(1,  untounched_trg.shape[0] * untounched_trg.shape[1] * untouched_src.shape[1])\n",
        "            alpha = alpha.view(1,  untounched_trg.shape[0] * untounched_trg.shape[1],  untouched_src.shape[1])\n",
        "            alpha = alpha.view(1,  untounched_trg.shape[0], untounched_trg.shape[1],  untouched_src.shape[1])\n",
        "            alpha = alpha.permute(1, 0, 2, 3)\n",
        "            alpha = alpha.view(untounched_trg.shape[0], untounched_trg.shape[1], untouched_src.shape[1])\n",
        "            #alpha = attention.sum(dim=1) # bsz x out_seq_len x in_seq_len # attention\n",
        "            \n",
        "            out_seq_len = alpha.shape[1]\n",
        "            in_seq_len = alpha.shape[2]\n",
        "            # mask input tokens that does not correspond to output tokens to -inf\n",
        "            mask = torch.zeros_like(alpha, requires_grad=False)\n",
        "            mask[torch.where(untounched_trg == self.trg_pad_idx)] = float('-inf') #  bsz x out_seq_len x in_seq_len\n",
        "            \n",
        "            mask = mask.permute(0, 2, 1)\n",
        "            mask[torch.where(untouched_src == self.src_pad_idx)] = float('-inf') #  bsz x in_seq_len x out_seq_len\n",
        "\n",
        "            mask = mask.permute(0, 2, 1) #  bsz x out_seq_len x in_seq_len\n",
        "\n",
        "            masked_alpha = alpha + mask\n",
        "            \n",
        "            #print(alpha.max())\n",
        "            #print(alpha.min())\n",
        "            concated = torch.cat((output, alpha), dim=2) # bsz x out_seq_len x (in_seq_len + len(output_types))\n",
        "\n",
        "            #concated = torch.nn.functional.softmax(concated, dim=2) # normalize\n",
        "            \n",
        "            #concated = torch.nn.functional.softmax(concated, dim=2) # normalize\n",
        "\n",
        "            normalized_input = concated[:,:,self.output_dim:] # bsz x out_seq_len x in_seq_len         probabilities for copy[]\n",
        "\n",
        "            normalized_output = concated[:,:,:self.output_dim] # bsz x out_seq_len x len(output_types) \n",
        "\n",
        "            mapped_input = torch.zeros_like(normalized_output)\n",
        "\n",
        "            ## replaced by scatter axis?\n",
        "            ## replaced by scatter axis?\n",
        "            # scatter_add\n",
        "            # dim: the axis starts to index\n",
        "            # indexes\n",
        "            # values\n",
        "            # xid: what is the input locations in the output vocabs\n",
        "            #pred = (g * pred).scatter_add(2, xids, (1 - g) * dists)\n",
        "            ## prepare x_id\n",
        "\n",
        "            # src_in_str = np.asarray(self.source_field.vocab.itos)[untouched_src.cpu().data.int().numpy()]\n",
        "            # src_to_trg_indices = [[self.target_field.vocab.stoi[e_word] for e_word in e_row] for e_row in src_in_str]\n",
        "\n",
        "            src_in_str = np.asarray(self.tp_itos)[untouched_src.cpu().data.int().numpy()]\n",
        "            src_to_trg_indices = [[self.target_field.getIndex(e_word) for e_word in e_row] for e_row in src_in_str]\n",
        "\n",
        "            src_to_trg_tensor = torch.Tensor(src_to_trg_indices).long().to(self.device)\n",
        "            \n",
        "            bsz = untouched_src.shape[0]\n",
        "            \n",
        "            xid = src_to_trg_tensor.view(bsz, 1, in_seq_len).repeat(1, out_seq_len, 1)\n",
        "            ###\n",
        "            mapped_input = mapped_input.scatter_add(2, xid, normalized_input)\n",
        "            final_output = normalized_output + mapped_input\n",
        "            output = final_output\n",
        "        \n",
        "        return output, attention\n",
        "\n",
        "\n",
        "\n",
        "class PretrainedReconstructorEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100,\n",
        "                 src_field=SOURCE,\n",
        "                 trg_field=TARGET):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        #self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        #self.tok_embedding = nn.Embedding.from_pretrained(src_field.vocab.vectors, freeze=False)\n",
        "        self.tok_embedding = nn.Linear(input_dim, hid_dim)\n",
        "        \n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, src_vocab_dim]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        src = self.tok_embedding(src.view(batch_size * src_len, src.shape[2]))\n",
        "        src =  src.view(batch_size, src_len, self.hid_dim)\n",
        "        src = self.dropout((src * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src\n",
        "    \n",
        "\n",
        "\n",
        "class AttentionPointerReconstructorSeq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 copy = True,\n",
        "                 output_dim = TARGET.get_vocab_size(),\n",
        "                 source_field = SOURCE,\n",
        "                 target_field = TARGET\n",
        "                ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        self.copy = copy\n",
        "        self.output_dim = output_dim\n",
        "        self.source_field = source_field\n",
        "        self.target_field = target_field\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, src_tensor, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src_tensor, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask, src)\n",
        "\n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djO_TyM_1VXU"
      },
      "source": [
        "# SQL2Text Model\n",
        "\n",
        "The SQL2Text Model contains the following function:\n",
        "\n",
        "1. train(epochs, batch_sz)\n",
        "2. evaluate(batch_sz, split='dev')\n",
        "3. train_with_reconstruct(epochs, batch_sz)\n",
        "4. evaluate_with_reconstruct(batch_sz, split='dev')\n",
        "5. translate_sample(batch_sz, sample_sz, random=False, split='test')\n",
        "6. translate_to_file(filename, batch_sz, split='test')\n",
        "7. translate_sample_post_copy(batch_sz, sample_sz, random=False, split='test')\n",
        "8. translate_to_file_post_copy(filename, batch_sz, split='test')\n",
        "\n",
        "Please use these functions accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnVMzLTOlkMu"
      },
      "outputs": [],
      "source": [
        "# This block only needs to be run once unless settings are tweaked.\n",
        "\n",
        "agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
        "cond_ops = ['=', '>', '<', 'OP']\n",
        "syms = ['SELECT', 'WHERE', 'AND', 'COL', 'TABLE', 'CAPTION', 'PAGE', 'SECTION', 'OP', 'COND', 'QUESTION', 'AGG', 'AGGOPS', 'CONDOPS']\n",
        "\n",
        "all_sql_syms = agg_ops + cond_ops + syms\n",
        "all_sql_syms = [e_sym.lower() for e_sym in all_sql_syms]\n",
        "\n",
        "# post copy using attention\n",
        "def post_copy_processing(src, pred, attention):\n",
        "    unk_locs = np.where(np.asarray(pred) == '<unk>')[0]\n",
        "    refined_sentence = deepcopy(pred)\n",
        "    exclude_idx = np.isin(np.asarray(src), np.asarray(all_sql_syms))\n",
        "    excluded_src = np.asarray(src)[~exclude_idx]\n",
        "    excluded_attention = attention[:, ~exclude_idx]\n",
        "\n",
        "    for e_unk_idx in unk_locs:\n",
        "        this_unk_attention = excluded_attention[e_unk_idx, :]\n",
        "        best_matched_inp_idx = this_unk_attention.argmax().cpu().data.numpy()\n",
        "        best_matched_inp = excluded_src[best_matched_inp_idx]\n",
        "        refined_sentence[e_unk_idx] = best_matched_inp\n",
        "        # set already matched to -inf\n",
        "        excluded_attention[:, best_matched_inp_idx] = 0\n",
        "    return ' '.join(refined_sentence)\n",
        "\n",
        "\n",
        "# Convert a list of token to a string, stop at first <eos>\n",
        "def wordid2str(word_ids, vocab):\n",
        "    ret = []\n",
        "    assert len(word_ids.shape) == 2, print(word_ids.shape)\n",
        "    for i in range(word_ids.shape[0]):\n",
        "        id_list = word_ids[i, :]\n",
        "        ret_inst = []\n",
        "        for j in range(id_list.shape[0]):\n",
        "            if id_list[j] == vocab.EOS:\n",
        "                break\n",
        "            token = vocab.getWord(id_list[j])\n",
        "            ret_inst.append(token)\n",
        "        ret.append(\" \".join(ret_inst))\n",
        "    return ret \n",
        "\n",
        "\n",
        "# Convert a list of token to a string\n",
        "def wordid2str_all(word_ids, vocab):\n",
        "    ret = []\n",
        "    assert len(word_ids.shape) == 2, print(word_ids.shape)\n",
        "    for i in range(word_ids.shape[0]):\n",
        "        id_list = word_ids[i, :]\n",
        "        ret_inst = []\n",
        "        for j in range(id_list.shape[0]):\n",
        "            # if id_list[j] == vocab.EOS:\n",
        "            #     break\n",
        "            token = vocab.getWord(id_list[j])\n",
        "            ret_inst.append(token)\n",
        "        ret.append(\" \".join(ret_inst))\n",
        "    return ret \n",
        "\n",
        "\n",
        "\n",
        "class SQL2TextModel:\n",
        "    def __init__(self, \n",
        "                 opt, \n",
        "                 vocab_model, \n",
        "                 train_dataset, \n",
        "                 dev_dataset, \n",
        "                 test_dataset=None):\n",
        "        \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # self.device = torch.device(\"cpu\")\n",
        "\n",
        "        # Data related\n",
        "        self.train_data = train_dataset\n",
        "        self.dev_data = dev_dataset\n",
        "        self.test_data = test_dataset\n",
        "        self.vocab_model = vocab_model\n",
        "\n",
        "        # Graph2Seq\n",
        "        self.opt = opt\n",
        "        self.enable_copy = self.opt['decoder_args']['rnn_decoder_share']['use_copy']\n",
        "        self.use_coverage = self.opt['decoder_args']['rnn_decoder_share']['use_coverage']\n",
        "        self.network = Graph2Seq.from_args(opt=self.opt, vocab_model=vocab_model).to(self.device)\n",
        "        self.loss = Graph2SeqLoss(ignore_index=self.vocab_model.out_word_vocab.PAD, \n",
        "                                  use_coverage=self.use_coverage, \n",
        "                                  coverage_weight=0.3)\n",
        "\n",
        "\n",
        "        self.CLIP = 5\n",
        "\n",
        "        # Reconstructor\n",
        "        INPUT_DIM = self.vocab_model.out_word_vocab.get_vocab_size()\n",
        "        OUTPUT_DIM = self.vocab_model.in_word_vocab.get_vocab_size()\n",
        "        HID_DIM = self.vocab_model.in_word_vocab.embeddings.shape[1]\n",
        "        ENC_LAYERS = 3\n",
        "        DEC_LAYERS = 3\n",
        "        ENC_HEADS = 10\n",
        "        DEC_HEADS = 10\n",
        "        ENC_PF_DIM = 512\n",
        "        DEC_PF_DIM = 512\n",
        "        ENC_DROPOUT = 0.1\n",
        "        DEC_DROPOUT = 0.1\n",
        "        SRC_PAD_IDX = self.vocab_model.in_word_vocab.PAD\n",
        "        TRG_PAD_IDX = self.vocab_model.out_word_vocab.PAD\n",
        "\n",
        "        self.enc = PretrainedReconstructorEncoder(INPUT_DIM, \n",
        "                                                  HID_DIM, \n",
        "                                                  ENC_LAYERS, \n",
        "                                                  ENC_HEADS, \n",
        "                                                  ENC_PF_DIM, \n",
        "                                                  ENC_DROPOUT, \n",
        "                                                  self.device,\n",
        "                                                  max_length = 100,\n",
        "                                                  src_field=self.vocab_model.out_word_vocab,\n",
        "                                                  trg_field=self.vocab_model.in_word_vocab)\n",
        "\n",
        "        self.dec = AttentionPointerDecoderV3(OUTPUT_DIM, \n",
        "                                             HID_DIM, \n",
        "                                             DEC_LAYERS, \n",
        "                                             DEC_HEADS, \n",
        "                                             DEC_PF_DIM, \n",
        "                                             DEC_DROPOUT, \n",
        "                                             self.device,\n",
        "                                             copy=True,\n",
        "                                             source_field = self.vocab_model.out_word_vocab,\n",
        "                                             target_field = self.vocab_model.in_word_vocab,\n",
        "                                             src_pad_idx=TRG_PAD_IDX, \n",
        "                                             trg_pad_idx=SRC_PAD_IDX, \n",
        "                                             max_length = 100)\n",
        "        \n",
        "        self.reconstructor = AttentionPointerReconstructorSeq2Seq(self.enc, self.dec, \n",
        "                                                                  TRG_PAD_IDX, SRC_PAD_IDX, \n",
        "                                                                  self.device, \n",
        "                                                                  copy = True, output_dim=OUTPUT_DIM,\n",
        "                                                                  source_field = self.vocab_model.out_word_vocab,\n",
        "                                                                  target_field = self.vocab_model.in_word_vocab).to(self.device)\n",
        "\n",
        "        self.recon_loss = nn.CrossEntropyLoss(ignore_index = 1)\n",
        "\n",
        "    def train(self, epochs, batch_sz):\n",
        "\n",
        "        dataset = SQL2TextDataset(self.train_data, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=True, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        best_perplexity = float('inf')\n",
        "\n",
        "        parameters = [p for p in self.network.parameters() if p.requires_grad]\n",
        "        learning_rate = 1e-3\n",
        "        optimizer = optim.Adam(parameters, lr=learning_rate)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss_collect = []\n",
        "            self.network.train()\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                # print(batch_graph, y)\n",
        "                batch_graph, y_idx, y_str = d['graph'], d['output_idx'], d['output_str']\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict, y = prepare_ext_vocab(x, self.vocab_model, gt_str=y_str, device=self.device)\n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "                loss = self.loss(logits=prob, label=y, enc_attn_weights=enc_attn_weights, coverage_vectors=coverage_vectors)\n",
        "                loss_collect.append(loss.item())\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for group in optimizer.param_groups for p in group['params']], self.CLIP)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "            perplexity = self.evaluate(batch_sz=batch_sz)\n",
        "\n",
        "            print('Epoch {}: \\n    Total loss: {:.3f} \\n    Dev set perplexity: {:.3f}'.format(epoch, np.sum(loss_collect), perplexity))\n",
        "            if perplexity <= best_perplexity:\n",
        "                print(\"Best model saved, epoch {}\".format(epoch))\n",
        "                torch.save(self.network.state_dict(), 'sql2text_best.pt')\n",
        "            else:\n",
        "                print(f'Perplexity is {perplexity}, it stops to drop at Epoch {epoch}, early stop.')\n",
        "                break\n",
        "\n",
        "            if abs(perplexity - best_perplexity) < 0.3:\n",
        "                learning_rate /= 2\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = learning_rate\n",
        "                print(f'Update learning rate to be {learning_rate} since the difference between perplexity and best perplexity is smaller than 0.3')\n",
        "\n",
        "            best_perplexity = min(best_perplexity, perplexity)\n",
        "\n",
        "        return best_perplexity\n",
        "\n",
        "\n",
        "    def evaluate(self, batch_sz, split='dev'):\n",
        "    \n",
        "        self.network.eval()\n",
        "\n",
        "        dataset = SQL2TextDataset(self.dev_data, self.vocab_model) if split == 'dev' else SQL2TextDataset(self.test_data, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=True, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        bsz = []\n",
        "        perplexity = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                batch_graph, y_idx = d['graph'], d['output_idx']\n",
        "\n",
        "                bsz.append(len(y_idx))\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)          \n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "                loss = self.loss(logits=prob, label=y, enc_attn_weights=enc_attn_weights, coverage_vectors=coverage_vectors)\n",
        "\n",
        "                # # print(enc_attn_weights.shape)\n",
        "                # print(enc_attn_weights[0].shape)\n",
        "                # print(len(enc_attn_weights), prob.shape[1])\n",
        "\n",
        "\n",
        "                perplexity += loss * len(y_idx)\n",
        "\n",
        "            perplexity /= sum(bsz)\n",
        "            perplexity = torch.exp(perplexity)\n",
        "\n",
        "        return perplexity.item()\n",
        "\n",
        "    # train with reconstruction loss\n",
        "    def train_with_reconstruct(self, epochs, batch_sz):\n",
        "        dataset = SQL2TextDataset(self.train_data, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=True, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        best_sql2text_loss = float('inf')\n",
        "\n",
        "        parameters = list(self.network.parameters()) + list(self.reconstructor.parameters())\n",
        "        learning_rate = 1e-3\n",
        "        weight_decay = 1e-4\n",
        "        optimizer = optim.Adam(parameters, lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss_collect = []\n",
        "            self.network.train()\n",
        "            self.reconstructor.train()\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                # print(batch_graph, y)\n",
        "                batch_graph, y_idx, y_str, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict, y = prepare_ext_vocab(x, self.vocab_model, gt_str=y_str, device=self.device)\n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "                pred_text = prob.argmax(dim=-1)\n",
        "\n",
        "                tp_src = [ [self.vocab_model.in_word_vocab.SOS] + self.vocab_model.in_word_vocab.to_index_sequence(s) + [self.vocab_model.in_word_vocab.EOS] for s in sql]\n",
        "                tp_src = pad_2d_vals_no_size(np.array(tp_src), dtype=np.int32)\n",
        "                src = torch.LongTensor(tp_src).to(self.device)\n",
        "                output_sql, _ = self.reconstructor(pred_text, prob, src[:,:-1])\n",
        "\n",
        "                output_sql_dim = output_sql.shape[-1]\n",
        "                output_sql = output_sql.contiguous().view(-1, output_sql_dim)\n",
        "\n",
        "                sql_trg = src[:,1:].contiguous().view(-1)  \n",
        "\n",
        "                loss = self.loss(logits=prob, \n",
        "                                 label=y, \n",
        "                                 enc_attn_weights=enc_attn_weights, \n",
        "                                 coverage_vectors=coverage_vectors) + self.recon_loss(output_sql, sql_trg)\n",
        "\n",
        "                loss_collect.append(loss.item() * 0.5)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_([p for group in optimizer.param_groups for p in group['params']], self.CLIP)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "            sql2text_loss, text2sql_loss, total_loss = self.evaluate_with_reconstruct(batch_sz=batch_sz)\n",
        "\n",
        "            print(f'Epoch {epoch}')\n",
        "            print('    Total train loss: {:.3f}'.format(np.sum(loss_collect)))\n",
        "            print(f'    SQL2Text Dev set ppl: {sql2text_loss}')\n",
        "            print(f'    Text2SQL Dev set ppl: {text2sql_loss}')\n",
        "            print(f'    Total Dev set ppl: {total_loss} \\n')\n",
        "            \n",
        "            if sql2text_loss <= best_sql2text_loss:\n",
        "                print(\"Best model saved, epoch {}\".format(epoch))\n",
        "                torch.save(self.network.state_dict(), 'sql2text_best.pt')\n",
        "                torch.save(self.reconstructor.state_dict(), 'text2sql_best.pt')\n",
        "            else:\n",
        "                print(f'SQL2Text Perplexity is {sql2text_loss}, it stops to drop at Epoch {epoch}, early stop.')\n",
        "                break\n",
        "\n",
        "            if abs(sql2text_loss - best_sql2text_loss) < 0.3:\n",
        "                learning_rate /= 10\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = learning_rate\n",
        "                print(f'Update learning rate to be {learning_rate} since the difference between perplexity and best perplexity is smaller than 0.3')\n",
        "\n",
        "            best_sql2text_loss = min(best_sql2text_loss, sql2text_loss)\n",
        "\n",
        "        return best_sql2text_loss\n",
        "\n",
        "\n",
        "    # eval with reconstruction loss\n",
        "    def evaluate_with_reconstruct(self, batch_sz, split='dev'):\n",
        "        self.network.eval()\n",
        "\n",
        "        dataset = SQL2TextDataset(self.dev_data, self.vocab_model) if split == 'dev' else SQL2TextDataset(self.test_data, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=True, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        bsz = []\n",
        "        sql2text_loss_total = 0\n",
        "        text2sql_loss_total = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                batch_graph, y_idx, _, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                bsz.append(len(y_idx))\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)          \n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "\n",
        "                one_hot_trg = torch.nn.functional.one_hot(y.view(y.shape[0] * y.shape[1]),  num_classes=self.vocab_model.out_word_vocab.get_vocab_size())\n",
        "                one_hot_trg = one_hot_trg.view(y.shape[0], y.shape[1], self.vocab_model.out_word_vocab.get_vocab_size()).float()\n",
        "\n",
        "                tp_src = [ [self.vocab_model.in_word_vocab.SOS] + self.vocab_model.in_word_vocab.to_index_sequence(s) + [self.vocab_model.in_word_vocab.EOS] for s in sql]\n",
        "                tp_src = pad_2d_vals_no_size(np.array(tp_src), dtype=np.int32)\n",
        "                src = torch.LongTensor(tp_src).to(self.device)\n",
        "\n",
        "                output_sql, _ = self.reconstructor(y, one_hot_trg, src[:,:-1])\n",
        "\n",
        "                output_sql_dim = output_sql.shape[-1]\n",
        "\n",
        "                output_sql = output_sql.contiguous().view(-1, output_sql_dim)\n",
        "\n",
        "                text_trg = y[:,1:].contiguous().view(-1)\n",
        "                sql_trg = src[:,1:].contiguous().view(-1)        \n",
        "                #output = [batch size * trg len - 1, output dim]\n",
        "                #trg = [batch size * trg len - 1]\n",
        "\n",
        "                sql2text_loss = self.loss(logits=prob, label=y, enc_attn_weights=enc_attn_weights, coverage_vectors=coverage_vectors)\n",
        "                text2sql_loss = self.recon_loss(output_sql, sql_trg)\n",
        "                loss = self.loss(logits=prob, label=y, enc_attn_weights=enc_attn_weights, coverage_vectors=coverage_vectors) + self.recon_loss(output_sql, sql_trg)\n",
        "                \n",
        "                \n",
        "                sql2text_loss_total += sql2text_loss.item() * len(y_idx)\n",
        "                text2sql_loss_total += text2sql_loss.item() * len(y_idx)\n",
        "                total_loss += loss.item() * 0.5 * len(y_idx)\n",
        "\n",
        "\n",
        "            sql2text_loss_total /= sum(bsz)\n",
        "            sql2text_loss_total = math.exp(sql2text_loss_total)\n",
        "            \n",
        "            text2sql_loss_total /= sum(bsz)\n",
        "            text2sql_loss_total = math.exp(text2sql_loss_total)\n",
        "\n",
        "            total_loss /= sum(bsz)\n",
        "            total_loss = math.exp(total_loss)\n",
        "\n",
        "        return sql2text_loss_total, text2sql_loss_total, total_loss\n",
        "    \n",
        "\n",
        "    # translate a sample of dataset\n",
        "    def translate_sample(self, batch_sz, sample_sz, random=False, split='test'):\n",
        "\n",
        "        assert batch_sz <= sample_sz, 'Batch size must be smaller or equal to the sample size.'\n",
        "        assert split == 'test' or split == 'dev', 'Sample must come from dev or test set.'\n",
        "        \n",
        "        self.network.eval()\n",
        "\n",
        "        datalist = self.test_data if split == 'test' else self.dev_data\n",
        "        assert len(datalist) >= sample_sz, f'Sample size exceed the number of data in {split} set.'\n",
        "\n",
        "        datalist = sample(datalist, sample_sz) if random else datalist[:sample_sz]\n",
        "        dataset = SQL2TextDataset(datalist, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=random, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        ex = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "\n",
        "                batch_graph, y_idx, y_str, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "                \n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)\n",
        "                    ref_dict = oov_dict\n",
        "                else:\n",
        "                    oov_dict = None\n",
        "                    ref_dict = self.vocab_model.out_word_vocab\n",
        "\n",
        "                pred = self.network.translate(batch_graph=x, oov_dict=oov_dict, beam_size=4, topk=1)\n",
        "                pred_ids = pred[:, 0, :]\n",
        "                pred_str = wordid2str(pred_ids.detach().cpu(), ref_dict)\n",
        "\n",
        "                for i in range(len(pred)):\n",
        "                    print(f'Sample {ex}')\n",
        "                    print(f'Original SQL: {sql[i]}')\n",
        "                    print(f'Original text: {y_str[i]}')\n",
        "                    print(f'Predicted text: {pred_str[i]} \\n')\n",
        "                    ex += 1\n",
        "\n",
        "    # translate all dataset and save to file\n",
        "    def translate_to_file(self, filename, batch_sz, split='test'):\n",
        "\n",
        "        assert split == 'test' or split == 'dev', 'Sample must come from dev or test set.'\n",
        "        \n",
        "        self.network.eval()\n",
        "\n",
        "        datalist = self.test_data if split == 'test' else self.dev_data\n",
        "        dataset = SQL2TextDataset(datalist, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=False, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        ex = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "\n",
        "                batch_graph, y_idx, y_str, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "                \n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)\n",
        "                    ref_dict = oov_dict\n",
        "                else:\n",
        "                    oov_dict = None\n",
        "                    ref_dict = self.vocab_model.out_word_vocab\n",
        "\n",
        "                pred = self.network.translate(batch_graph=x, oov_dict=oov_dict, beam_size=4, topk=1)\n",
        "                pred_ids = pred[:, 0, :]\n",
        "                pred_str = wordid2str(pred_ids.detach().cpu(), ref_dict)\n",
        "\n",
        "                for i in range(len(pred)):\n",
        "                    f.write(f'Sample {ex}\\n')\n",
        "                    f.write(f'Original SQL: {sql[i]}\\n')\n",
        "                    f.write(f'Original text: {y_str[i]}\\n')\n",
        "                    f.write(f'Predicted text: {pred_str[i]} \\n\\n')\n",
        "                    ex += 1\n",
        "\n",
        "        f.close()\n",
        "\n",
        "\n",
        "    def translate_sample_post_copy(self, batch_sz, sample_sz, random=False, split='test'):\n",
        "        assert batch_sz <= sample_sz, 'Batch size must be smaller or equal to the sample size.'\n",
        "        assert split == 'test' or split == 'dev', 'Sample must come from dev or test set.'\n",
        "        \n",
        "        self.network.eval()\n",
        "\n",
        "        datalist = self.test_data if split == 'test' else self.dev_data\n",
        "        assert len(datalist) >= sample_sz, f'Sample size exceed the number of data in {split} set.'\n",
        "\n",
        "        datalist = sample(datalist, sample_sz) if random else datalist[:sample_sz]\n",
        "        dataset = SQL2TextDataset(datalist, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=random, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        ex = 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                batch_graph, y_idx, y_str, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                # print(x.node_features)\n",
        "                # print(x.node_attributes)\n",
        "                # print(from_batch(x)[0].node_attributes)\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)          \n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "                pred_ids = prob.argmax(dim=-1)\n",
        "                pred_text = wordid2str_all(pred_ids.detach().cpu(), self.vocab_model.out_word_vocab)\n",
        "\n",
        "                attn = torch.concat(enc_attn_weights, dim=0)\n",
        "                attn = attn.permute(1, 0, 2)\n",
        "\n",
        "                origin_g = from_batch(x)\n",
        "\n",
        "                for i in range(len(pred_text)):\n",
        "                    # print(len(sql[i].split()), len(pred_text[i].split()))\n",
        "                    # print(attn[i].shape)\n",
        "                    # print(pred_ids[i])\n",
        "                    # print(attn[i].shape)\n",
        "                    tp = [d['token'] for d in origin_g[i].node_attributes]\n",
        "                    res = post_copy_processing(tp + ['#pad#'] * (attn[i].shape[1] - len(tp)), pred_text[i].split(), attn[i])\n",
        "                    res = res[:res.find('</s>')] if res.find('</s>') else res\n",
        "                    print(f'Sample {ex}')\n",
        "                    print(f'Original SQL: {sql[i]}')\n",
        "                    print(f'Original text: {y_str[i]}')\n",
        "                    # print(f'Predicted text: {pred_text[i]}')\n",
        "                    print(f'Predicted text after post copy: {res} \\n')\n",
        "\n",
        "                    ex += 1\n",
        "\n",
        "    def translate_to_file_post_copy(self, filename, batch_sz, split='test'):\n",
        "        assert split == 'test' or split == 'dev', 'Sample must come from dev or test set.'\n",
        "        \n",
        "        self.network.eval()\n",
        "\n",
        "        datalist = self.test_data if split == 'test' else self.dev_data\n",
        "        dataset = SQL2TextDataset(datalist, self.vocab_model)\n",
        "        loader = DataLoader(dataset, batch_size=batch_sz, shuffle=False, collate_fn=SQL2TextDataset.collate_fn)\n",
        "\n",
        "        ex = 1\n",
        "        f = open(filename, \"w\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for d in tqdm.notebook.tqdm(loader, leave=False):\n",
        "                batch_graph, y_idx, y_str, sql = d['graph'], d['output_idx'], d['output_str'], d['query']\n",
        "\n",
        "                x, y = batch_graph.to(self.device), y_idx.to(self.device)\n",
        "\n",
        "                oov_dict = None\n",
        "                if self.enable_copy:\n",
        "                    oov_dict = prepare_ext_vocab(batch_graph=x, vocab=self.vocab_model, device=self.device)          \n",
        "\n",
        "                prob, enc_attn_weights, coverage_vectors = self.network(x, y, oov_dict=oov_dict)\n",
        "                pred_ids = prob.argmax(dim=-1)\n",
        "                pred_text = wordid2str_all(pred_ids.detach().cpu(), self.vocab_model.out_word_vocab)\n",
        "\n",
        "                attn = torch.concat(enc_attn_weights, dim=0)\n",
        "                attn = attn.permute(1, 0, 2)\n",
        "\n",
        "                origin_g = from_batch(x)\n",
        "\n",
        "                for i in range(len(pred_text)):\n",
        "                    tp = [d['token'] for d in origin_g[i].node_attributes]\n",
        "                    res = post_copy_processing(tp + ['#pad#'] * (attn[i].shape[1] - len(tp)), pred_text[i].split(), attn[i])\n",
        "                    res = res[:res.find('</s>')] if res.find('</s>') else res\n",
        "                    f.write(f'Sample {ex}\\n')\n",
        "                    f.write(f'Original SQL: {sql[i]}\\n')\n",
        "                    f.write(f'Original text: {y_str[i]}\\n')\n",
        "                    f.write(f'Predicted text after post copy: {res} \\n\\n')\n",
        "\n",
        "                    ex += 1\n",
        "        f.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-1PAC6USXP6"
      },
      "source": [
        "## Train with Reconstruction Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWwK4qfCuOwf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model_recon = SQL2TextModel(opt, vocab_model, train_data_set, dev_dataset, test_dataset)\n",
        "\n",
        "print(model_recon.network.gnn_encoder)\n",
        "print(model_recon.network.seq_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibJoIzflTp1a"
      },
      "outputs": [],
      "source": [
        "model_recon.train_with_reconstruct(epochs=10, batch_sz=64)\n",
        "# model.evaluate_with_reconstruct(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LERa-VlvTfQY"
      },
      "outputs": [],
      "source": [
        "# print a sample of translation\n",
        "model.network.load_state_dict(torch.load('sql2text_best.pt'))\n",
        "model.translate_sample(batch_sz=8, sample_sz=32, split='dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xa4rcQu4w5f"
      },
      "outputs": [],
      "source": [
        "model.evaluate_with_reconstruct(64, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pquhmML9Sg0E"
      },
      "source": [
        "## Train with NLL Loss and Translate a Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlWJQPokhFSN"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = SQL2TextModel(opt, vocab_model, train_data_set, dev_dataset, test_dataset)\n",
        "\n",
        "print(model.network.gnn_encoder)\n",
        "print(model.network.seq_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1lZKfSpOJLe"
      },
      "outputs": [],
      "source": [
        "model.train(epochs=10, batch_sz=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lai2ipDQssj"
      },
      "outputs": [],
      "source": [
        "model.network.load_state_dict(torch.load('sql2text_best.pt'))\n",
        "\n",
        "# model.translate_sample(batch_sz=8, sample_sz=32, split='dev')\n",
        "model.translate_sample_post_copy(batch_sz=8, sample_sz=32, split='dev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLq6bsxD9Q5"
      },
      "outputs": [],
      "source": [
        "model.evaluate(64, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ7Mm_CLNq3T"
      },
      "source": [
        "# Predict and Write to File\n",
        "\n",
        "Please change the file name before running it. Please also review the instruction under SQL2Text Model to know each functions can be called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NyN9DXO-_W9"
      },
      "outputs": [],
      "source": [
        "# change this accordingly\n",
        "fname1 = 'drive/MyDrive/CompSci-590-NLP/Final Project/graph2seq_test_set_translate.txt'\n",
        "fname2 = 'drive/MyDrive/CompSci-590-NLP/Final Project/graph2seq_test_set_translate_post_copy.txt'\n",
        "\n",
        "model = SQL2TextModel(opt, vocab_model, train_data_set, dev_dataset, test_dataset)\n",
        "model.network.load_state_dict(torch.load('sql2text_best.pt'))\n",
        "# model.reconstructor.load_state_dict(torch.load('text2sql_best.py'))\n",
        "model.translate_to_file(fname1, 64, split='test')\n",
        "model.translate_to_file_post_copy(filename=fname2, batch_sz=64, split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spjXwf5WQxPO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zR4GqepjMD7"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This section is used for random evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Iph1iZrtqZIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86fc0e8-00cc-4a87-b105-5a7a97fde472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15878 15878 15878\n"
          ]
        }
      ],
      "source": [
        "target = []\n",
        "g2s_pred = []\n",
        "s2s_pred = []\n",
        "sql = []\n",
        "\n",
        "with open(\"sql2text-seq2seq.txt\") as f1:\n",
        "    offset1 = len('Original Text:')\n",
        "    offset2 = len('Predicted Pred:')\n",
        "    offset3 = len('Original SQL:')\n",
        "    for line in f1:\n",
        "        if line.startswith('Original Text'):\n",
        "            s = line[offset1:].strip().split()\n",
        "            target.append([s])\n",
        "        elif line.startswith('Predicted Pred:'):\n",
        "            s = line[offset2:].strip().split()\n",
        "            s2s_pred.append(s)\n",
        "        elif line.startswith('Original SQL:'):\n",
        "            sql.append(line[offset3:].strip())\n",
        "        \n",
        "\n",
        "with open(\"sql2text-graph2seq.txt\") as f2:\n",
        "    offset1 = len('Original text:')\n",
        "    offset2 = len('Predicted text after post copy:')\n",
        "    for line in f2:\n",
        "        # if line.startswith('Original text:'):\n",
        "        #     s = line[offset1:].strip().split()\n",
        "        #     target.append(s)\n",
        "        if line.startswith('Predicted text after post copy:'):\n",
        "            s = line[offset2:].strip().split()\n",
        "            g2s_pred.append(s)\n",
        "        \n",
        "print(len(target), len(g2s_pred), len(s2s_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score as bs\n",
        "\n",
        "# output the queries that Graph2Seq performs better than Seq2Seq\n",
        "with open('graph-better.txt', 'w') as f:\n",
        "    for i in range(len(target)):\n",
        "        s1 = bs([s2s_pred[i]], [target[i]])\n",
        "        s2 = bs([g2s_pred[i]], [target[i]])\n",
        "        if s2 > s1:\n",
        "            f.write(f'Sample {i}\\n')\n",
        "            f.write(f'SQL: {sql[i]}\\n')\n",
        "            f.write(f'graph2seq: {\" \".join(g2s_pred[i])}\\n')\n",
        "            f.write(f'seq2seq: {\" \".join(s2s_pred[i])}\\n\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "fn3fV-_j_gAG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zXpCX087IPa8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1NY0JlXJlgaQ",
        "V-1PAC6USXP6"
      ],
      "name": "SQL2Text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}