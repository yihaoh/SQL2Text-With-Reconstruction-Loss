opt = {
  "graph_construction_args": {
    "graph_construction_share": {
      "graph_type": "node_emb",
      "root_dir": None,
      "topology_subdir": "NodeEmbGraph",
      "share_vocab": SHARE_VOCAB
    },
    "graph_construction_private": {
      "lower_case": False
    },
    "node_embedding": {
      "input_size": 512,
      "hidden_size": 512,
      "word_dropout": 0,
      "rnn_dropout": 0,
      "fix_bert_emb": False,
      "fix_word_emb": False,
      "embedding_style": {
        "single_token_item": True,
        "emb_strategy": "w2v_bilstm",
        "num_rnn_layers": 1,
        "bert_model_name": None,
        "bert_lower_case": None
      },
      "sim_metric_type": "weighted_cosine",
      "num_heads": 1,
      "top_k_neigh": 8,
    #   "epsilon_neigh": None,
    #   "smoothness_ratio": 0.1,
    #   "connectivity_ratio": 0.05,
    #   "sparsity_ratio": 0.1
    }
  },
  "graph_embedding_args": {
    "graph_embedding_share": {
      "num_layers": 3,
      "input_size": 512,
      "hidden_size": 512,
      "output_size": 512,
      "direction_option": "bi_sep",
      "feat_drop": 0.3
    },
    "graph_embedding_private": {
      "heads": [
        10, 10, 10
      ],
      "attn_drop": 0.4,
      "negative_slope": 0.2,
      "residual": False,
      "activation": "relu",
      "allow_zero_in_degree": False
    }
  },
  "decoder_args": {
    "rnn_decoder_share": {
      "rnn_type": "lstm",
      "input_size": 512,
      "hidden_size": 512,
      "rnn_emb_input_size": 512,
      "use_copy": False,
      "use_coverage": False,
      "graph_pooling_strategy": "max",
      "attention_type": "sep_diff_encoder_type",
      "fuse_strategy": "concatenate",
      "dropout": 0.4
    },
    "rnn_decoder_private": {
      "max_decoder_step": 50,
      "node_type_num": None,
      "tgt_emb_as_output_layer": False,
      "teacher_forcing_rate": 1
    }
  },
  "graph_construction_name": "node_emb",
  "graph_embedding_name": "graphsage",
  "decoder_name": "stdrnn"
}