{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e547e3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import datetime\n",
    "import utils\n",
    "from modeling.model_factory import create_model\n",
    "from featurizer import HydraFeaturizer, SQLDataset\n",
    "from evaluator import HydraEvaluator\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator, Pipeline, RawField\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f93fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcf4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SQL': ['select', 'notes', 'from', 'table', 'where', 'current', 'slogan', '=', 'south', 'australia'], 'text': ['<sos>', 'tell', 'me', 'what', 'the', 'notes', 'are', 'for', 'south', 'australia', '<eos>']}\n"
     ]
    }
   ],
   "source": [
    "# create Field objects\n",
    "SOURCE = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            tokenize = tokenizer,\n",
    "            lower=True, batch_first = False)\n",
    "TARGET = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            tokenize = tokenizer,\n",
    "            lower=True,  batch_first = False)\n",
    "\n",
    "# create tuples representing the columns\n",
    "fields = [\n",
    "  ('SQL', SOURCE),\n",
    "  ('text', TARGET),\n",
    "]\n",
    "\n",
    "# load the dataset in json format\n",
    "train_ds, valid_ds, test_ds = TabularDataset.splits(\n",
    "   path = './',\n",
    "   train = 'train_sql_text.csv',\n",
    "   validation = 'dev_sql_text.csv',\n",
    "   test = 'test_sql_text.csv',\n",
    "   format = 'csv',\n",
    "   fields = fields,\n",
    "   skip_header = True\n",
    ")\n",
    "\n",
    "# check an example\n",
    "print(vars(train_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b0a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_ds, min_freq = 25)\n",
    "TARGET.build_vocab(train_ds, min_freq = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba1e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what device to use\n",
    "device = torch.device(\n",
    "  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "#device = \"cpu\"\n",
    "# create iterators for train/valid/test datasets\n",
    "train_it, valid_it, test_it = BucketIterator.splits(\n",
    "  (train_ds, valid_ds, test_ds),\n",
    "  sort_key = lambda x: len(x.SQL),\n",
    "  sort = False,\n",
    "  batch_size = 128,\n",
    "  device = device\n",
    ")\n",
    "\n",
    "# iterate over training\n",
    "for batch in train_it:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c110269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc_hidden(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        cell = torch.tanh(self.fc_cell(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim = 1)))\n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #cell = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, (hidden.squeeze(0), cell.squeeze(0)), a.squeeze(1)\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, (hidden, cell) = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4799c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "INPUT_DIM = len(SOURCE.vocab)\n",
    "OUTPUT_DIM = len(TARGET.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32abb92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1464, 256)\n",
       "    (rnn): LSTM(256, 512, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(1621, 256)\n",
       "    (rnn): LSTM(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=1621, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2321b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,361,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471cbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SQL\n",
    "        trg = batch.text\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SQL\n",
    "            trg = batch.text\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d80fc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TARGET.vocab.stoi[TARGET.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5431901",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d86f75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 39s\n",
      "\tTrain Loss: 3.433 | Train PPL:  30.960\n",
      "\t Val. Loss: 3.500 |  Val. PPL:  33.107\n",
      "Epoch: 02 | Time: 1m 36s\n",
      "\tTrain Loss: 2.602 | Train PPL:  13.492\n",
      "\t Val. Loss: 3.266 |  Val. PPL:  26.210\n",
      "Epoch: 03 | Time: 1m 35s\n",
      "\tTrain Loss: 2.204 | Train PPL:   9.058\n",
      "\t Val. Loss: 3.202 |  Val. PPL:  24.594\n",
      "Epoch: 04 | Time: 1m 34s\n",
      "\tTrain Loss: 2.004 | Train PPL:   7.415\n",
      "\t Val. Loss: 3.132 |  Val. PPL:  22.911\n",
      "Epoch: 05 | Time: 1m 35s\n",
      "\tTrain Loss: 1.835 | Train PPL:   6.266\n",
      "\t Val. Loss: 3.119 |  Val. PPL:  22.614\n",
      "Epoch: 06 | Time: 1m 34s\n",
      "\tTrain Loss: 1.728 | Train PPL:   5.630\n",
      "\t Val. Loss: 3.148 |  Val. PPL:  23.280\n",
      "Epoch: 07 | Time: 1m 36s\n",
      "\tTrain Loss: 1.658 | Train PPL:   5.247\n",
      "\t Val. Loss: 3.156 |  Val. PPL:  23.479\n",
      "Epoch: 08 | Time: 1m 36s\n",
      "\tTrain Loss: 1.583 | Train PPL:   4.871\n",
      "\t Val. Loss: 3.215 |  Val. PPL:  24.910\n",
      "Epoch: 09 | Time: 1m 35s\n",
      "\tTrain Loss: 1.524 | Train PPL:   4.589\n",
      "\t Val. Loss: 3.194 |  Val. PPL:  24.381\n",
      "Epoch: 10 | Time: 1m 35s\n",
      "\tTrain Loss: 1.487 | Train PPL:   4.425\n",
      "\t Val. Loss: 3.271 |  Val. PPL:  26.330\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_it, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_it, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '/datacommons/carin/fk43/CS590/models/LSTM-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab77322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/datacommons/carin/fk43/CS590/models/LSTM-model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd2c789",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2961409/736811409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2961409/2897075056.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#turn off teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m#trg = [trg len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2961409/1898361341.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m#insert input token embedding, previous hidden state and all encoder hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss = evaluate(model, test_it, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33762428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "\n",
    "    tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    src_len = torch.LongTensor([len(src_indexes)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, (hidden, cell), attention = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
    "\n",
    "        attentions[i] = attention\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf2f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    \n",
    "    x_ticks = [''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>']\n",
    "    y_ticks = [''] + translation\n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95b2e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['select', 'school/club', 'team', 'from', 'table', 'where', 'no', '.', '=', '6']\n",
      "trg = ['<sos>', 'what', 'school', 'did', 'player', 'number', '6', 'come', 'from', '?', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 12\n",
    "\n",
    "src = vars(train_ds.examples[example_idx])['SQL']\n",
    "trg = vars(train_ds.examples[example_idx])['text']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "425900b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['<sos>', 'what', 'is', 'the', 'team', 'of', '6', '.', '6', '?', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate_sentence(src, SOURCE, TARGET, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4808baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/hpc/group/carin/fk43/anaconda3/envs/tfboys3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display_attention(src, translation, attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb2ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "cond_ops = ['=', '>', '<', 'OP']\n",
    "syms = ['SELECT', 'WHERE', 'AND', 'COL', 'TABLE', 'CAPTION', 'PAGE', 'SECTION', 'OP', 'COND', 'QUESTION', 'AGG', 'AGGOPS', 'CONDOPS']\n",
    "\n",
    "all_sql_syms = agg_ops + cond_ops + syms\n",
    "all_sql_syms = [e_sym.lower() for e_sym in all_sql_syms]\n",
    "\n",
    "def post_copy_processing(src, pred, attention):\n",
    "    unk_locs = np.where(np.asarray(pred) == '<unk>')[0]\n",
    "    refined_sentence = copy.deepcopy(pred)\n",
    "    exclude_idx = np.isin(np.asarray(src), np.asarray(all_sql_syms))\n",
    "    excluded_src = np.asarray(src)[~exclude_idx]\n",
    "    excluded_attention = attention[:, ~exclude_idx]\n",
    "\n",
    "    for e_unk_idx in unk_locs:\n",
    "        this_unk_attention = excluded_attention[e_unk_idx, :]\n",
    "        best_matched_inp_idx = this_unk_attention.argmax().cpu().data.numpy()\n",
    "        best_matched_inp = excluded_src[best_matched_inp_idx]\n",
    "        refined_sentence[e_unk_idx] = best_matched_inp\n",
    "        # set already matched to -inf\n",
    "        excluded_attention[:, best_matched_inp_idx] = 0\n",
    "    return ' '.join(refined_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6d403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b164e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ec229fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15878it [03:18, 80.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tqdm \n",
    "\n",
    "myfile = open('LSTM-sql2text_results_on_test_set2.txt', 'w')\n",
    "\n",
    "show_num_of_sample = 200\n",
    "src_field = SOURCE\n",
    "trg_field = TARGET\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    for i, sample in tqdm.tqdm(enumerate(test_ds)):\n",
    "\n",
    "        src = sample.SQL\n",
    "        trg = sample.text\n",
    "        pred_text, attention = translate_sentence(src, src_field, trg_field, model, device, max_len=100)\n",
    "        pred_text = post_copy_processing(src, pred_text[1:-1], attention[1:-1, 0, 1:-1])\n",
    "        ref_text = ' '.join(np.asarray(TARGET.vocab.itos)[[TARGET.vocab.stoi[e_word] for e_word in trg]])\n",
    "        #print(f'Referenced trg = {ref_text}')        \n",
    "        sql = ' '.join(src)\n",
    "        ref_text = ' '.join(trg[1:-1])\n",
    "#         print('-'*10 + 'Example {}'.format(i+1) + '-'*10)\n",
    "#         print(\"Original SQL: \" + sql.replace('<sos>', '').replace('<eos>', ''))\n",
    "#         print(\"Original Text: \" + ref_text)\n",
    "#         #print(\"Predicted Text: \" + pred_text.replace('<sos>', '').replace('<eos>', ''))\n",
    "#         print(\"Predicted Pred:\" + pred_text.replace('<sos>', '').replace('<eos>', ''))\n",
    "#         print('   ')\n",
    "        line_1 = '-'*10 + 'Example {}'.format(i+1) + '-'*10\n",
    "        line_2 = \"Original SQL: \" + sql.replace('<sos>', '').replace('<eos>', '')\n",
    "        line_3 = \"Original Text: \" + ref_text\n",
    "        line_4 = \"Predicted Pred:\" + pred_text.replace('<sos>', '').replace('<eos>', '')\n",
    "        myfile.write(\"%s\\n\" % line_1)\n",
    "        myfile.write(\"%s\\n\" % line_2)\n",
    "        myfile.write(\"%s\\n\" % line_3)\n",
    "        myfile.write(\"%s\\n\" % line_4)\n",
    "        myfile.write(\"%s\\n\" % '    ')\n",
    "myfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c15b5a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'nationality',\n",
       " 'of',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 'from',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '?',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28016d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
